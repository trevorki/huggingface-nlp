{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646d6f47-1ed6-40e1-a20a-45f15c6e71c4",
   "metadata": {},
   "source": [
    "# Fine-Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1040e130-8432-4843-a696-f25a86b0e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The basic procedure:\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# # Same as before\n",
    "# checkpoint = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# sequences = [\n",
    "#     \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "#     \"This course is amazing!\",\n",
    "# ]\n",
    "# batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\"))\n",
    "\n",
    "# This is new\n",
    "# model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "# labels = tf.convert_to_tensor([1, 1])\n",
    "# model.train_on_batch(batch, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c960f79a-0a47-4d34-b9af-2eb7e915cd7f",
   "metadata": {},
   "source": [
    "But.... this only trained the model on 2 examples, so not great. We need more data to tune on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8a9670-779a-4b6b-bc1e-e905efc64774",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "Data used for training models must be in dataset format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0357fa3d-019f-47c2-88b5-6f87c67e8d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# glue ( Generalized Language Understanding Evaluation)  is a bunch of datasets, we want the mrpc (Microsoft Research Paraphrase Corpus) part\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\") \n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdf8b0f-d6cc-46b4-a004-d646418867de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at a training example\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30910144-7ffd-4c22-bed0-4b3a556ba917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63cb8413-68a6-49a2-b241-427338f21eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: Dataset elements can be accessed by feature OR by index\n",
    "\n",
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eab6421-b9b8-4757-95ed-527c0ea7f357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"sentence1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "817d2469-ae9c-4511-b7b6-8a8fc06fe83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][\"sentence1\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b85c0-9bb2-47d8-9f93-58f9097f1af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a441674-44db-45e6-9b16-99cc592f755e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc9d42-1d94-46fe-a597-c41961aa832e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1874e7b-e71d-46a7-8484-8a401ddfb780",
   "metadata": {},
   "source": [
    "## Preprocessing sentence pairs\n",
    "Basic Idea: instead of passing a single sentence, pass a pair (tuple) to the tokenizer.. The tokenizer will output a `token_type_id` that for each token that represents which sentence it is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa5f687-dd93-4445-9b83-c9afb8299d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\", padding=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58797f6-1deb-43b9-9276-eb350d142bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6e93c-c4a4-4f7d-a730-a253b34b12d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c4c5b99-e9bb-41ad-ae42-f0f2c8364cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=25, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing them one at a time does not link them\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n",
    "\n",
    "tokenized_sentences_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ee78f6-dbc7-4d59-b2ae-6c485751cbe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing them together gives gives a list of tokens for both sentences together plus a list `token_type_ids` indicating which sentence each sentence came from\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    # padding=True,\n",
    "    # truncation=True,\n",
    ")\n",
    "tokenized_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e71207-6cb8-4e0d-9c4b-c0b449e24509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['input_ids'][0])\n",
    "print(tokenized_dataset['token_type_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c02676-e4eb-43c2-9cc9-1e48a5454723",
   "metadata": {},
   "source": [
    "**Problem:** This returns a dict  with keys: ['input_ids', 'token_type_ids', 'attention_mask'], which is inefficient and uses tons of memory.\n",
    "\n",
    "**A better way** is to map a function to the dataset because the dataset is an group of Apache Arrow files not in active menory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17713a7f-8c3d-4d88-8033-9496c6aa0073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a39a337f0c42ecb3aa219cc2e22f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "651f0cc1-a587-4ffd-a62a-fc2c517bd1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"]['input_ids'][0])\n",
    "print(tokenized_datasets[\"train\"]['token_type_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b94682-b7d5-4e28-8f0f-56f35e6fb49b",
   "metadata": {},
   "source": [
    "# Padding, revisited\n",
    "When using CPU or GPU, the batches don't all need the same size as long as within the batch they are equal, (but TPUs all input sizes must be equal)\n",
    "\n",
    "To do this we use **Dynamic Padding**, which groups similar-sized inputs together and applies batching to the max length in that batch. This makes training on CPU or GPU faster.\n",
    "\n",
    "We use a `DataCollatorWithPadding` for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "812e8259-d3d6-4389-b84e-14643cc9b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2193355-540c-4ff1-be61-46803f68cc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first 8 examples has wildly different lengths\n",
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9eb1cc93-45d2-4907-82c1-4b34ef12e50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': TensorShape([8, 67]),\n",
       " 'token_type_ids': TensorShape([8, 67]),\n",
       " 'attention_mask': TensorShape([8, 67]),\n",
       " 'labels': TensorShape([8])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a datacollator, will pad all examples to max size\n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ce7345c-048b-42f4-9162-902d873e2abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\datasets\\arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Now apply to entire dataset with `.to_tf_dataset` which will pad batches based on the `batch_size` parameter\n",
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81c7e131-edfa-4598-a2f1-ae49c6cd11a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773709cd-e543-4708-81a7-5c96ff4cc8db",
   "metadata": {},
   "source": [
    "# Fine Tuning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643b40f-8ccb-42d1-8289-fff8a5733458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "588d8c94-3a47-443c-bd72-3ae34becffb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "# load model from checkpoint, this will import the base model+weights...\n",
    "# AND add the appropriate layers for the head, depending on the application type and num_classes\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d080c507-4421-4a59-94cc-24ab71d57839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# note to specify the loss by function, not string (eg. \"sparsecategoricacrossentropy\"), \n",
    "# which will assume outputs are probabilities, not logits\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29b56576-65c9-4c32-89d9-2565127a5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this takes FOREVER!!! Like 15 minutes per epoch\n",
    "# model.fit(\n",
    "#     tf_train_dataset,\n",
    "#     validation_data=tf_validation_dataset,\n",
    "#     epochs = 1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a055da-0846-45bb-ba1f-2a2a737461fe",
   "metadata": {},
   "source": [
    "# To speed up training\n",
    "- lower learning rate from 0.001 to 0.0005\n",
    "- use learning rate decay using `PolynomialDecay` learning rate scheduler\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d8b346b-afb6-4d8b-a60e-75cfb8132ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
    "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
    "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "\n",
    "opt = Adam(learning_rate=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "623a6e91-8196-40d7-a886-466d5e6b1d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee6e22b0-f57b-4540-b9d1-5ff7f71cb06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "459/459 [==============================] - 881s 2s/step - loss: 0.5896 - accuracy: 0.7058 - val_loss: 0.4558 - val_accuracy: 0.7941\n",
      "Epoch 2/3\n",
      "459/459 [==============================] - 884s 2s/step - loss: 0.3486 - accuracy: 0.8457 - val_loss: 0.3476 - val_accuracy: 0.8456\n",
      "Epoch 3/3\n",
      "459/459 [==============================] - 868s 2s/step - loss: 0.1083 - accuracy: 0.9637 - val_loss: 0.4062 - val_accuracy: 0.8554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x190c135c6d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still takes ~12 minutes/epoch\n",
    "model.fit(tf_train_dataset, \n",
    "          validation_data=tf_validation_dataset, \n",
    "          epochs=3, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70d2cb1b-11a1-4677-a556-5b128d08dafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 27s 463ms/step\n",
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "preds = model.predict(tf_validation_dataset)[\"logits\"]\n",
    "# probs = tf.nn.softmax(preds) # if you want to use the probabilities\n",
    "\n",
    "class_preds = np.argmax(preds, axis=1)\n",
    "print(preds.shape, class_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82dc67b1-ffa1-437e-b31f-01d419b87559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8553921568627451, 'f1': 0.9001692047377327}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=class_preds, references=raw_datasets[\"validation\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268908b8-d6d4-4550-aa7b-fbc7bdfa48d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hugging]",
   "language": "python",
   "name": "conda-env-hugging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
