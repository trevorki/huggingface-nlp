{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6673c5dd-9ea9-4159-ab3f-79cd106af911",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "How to work with local and remote datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb868bd-5d6b-4b1d-9a25-b628ec62287e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "# download json datasets (large-scale dataset for question answering in Italian)\n",
    "!curl -0 https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!curl -0 https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
    "\n",
    "# !gzip -dkv SQuAD_it-*.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e80744d-651a-4ce4-9ce4-51a3902aae1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# can load just a single file\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=\"data/SQuAD_it-train.json\", field=\"data\")\n",
    "\n",
    "# or many files using a dict\n",
    "data_files = {\n",
    "    \"train\": \"data/SQuAD_it-train.json\", \n",
    "    \"test\": \"data/SQuAD_it-test.json\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17d7255-8f11-4891-807b-5b07f4cb2292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or even decompress files (damn)\n",
    "data_files = {\n",
    "    \"train\": \"data/SQuAD_it-train.json.gz\", \n",
    "    \"test\": \"data/SQuAD_it-test.json.gz\"\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa99eae9-eec9-4ebc-8f52-b2870c8998b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or even a remote dataset (DAMN!)\n",
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    \"train\": url + \"SQuAD_it-train.json.gz\",\n",
    "    \"test\": url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f13835-c5fd-4e18-88b8-ac00f26a3ac9",
   "metadata": {},
   "source": [
    "# Manipulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7134a110-04b7-4905-a435-1e0b004ddbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Drug Review Dataset\n",
    "# !wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "# !unzip drugsCom_raw.zip\n",
    "# !mv drugs* data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73df1a8-87b1-4fe0-9e7d-b7a9e02c4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\"train\": \"data/drugsComTrain_raw.tsv\", \"test\": \"data/drugsComTest_raw.tsv\"}\n",
    "# \\t is the tab character in Python\n",
    "drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32338d21-eebe-4a8a-8c8b-b23b9938dbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f2bcb4d-8cee-4dae-adab-cbf5efcb2fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at a small sample of them\n",
    "drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "drug_sample[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c7b305-ad56-405f-b29f-e63b65a36f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"Unnamed: 0\" column seems to be a unique patient id so check this\n",
    "for split in drug_dataset.keys():\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bab060f-7089-4ada-a6e9-f839b83af88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 161297\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
       "        num_rows: 53766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename \"Unnamed: 0\" to \"patient_id\"\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n",
    ")\n",
    "drug_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ff8b114-afb7-47fe-aa97-848ac8f7d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all `condotion` entries to lowercase by making a function to map onto dataset\n",
    "# Returns a dict with the new value of the key to replace\n",
    "def lowercase_condition(example):\n",
    "    return {\"condition\": example[\"condition\"].lower()}\n",
    "\n",
    "\n",
    "# # try to map it but uh-oh there are some null values\n",
    "# drug_dataset.map(lowercase_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe6620d-6247-43e5-9aa3-d61a5af01a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': 206461,\n",
       " 'drugName': 'Valsartan',\n",
       " 'condition': 'left ventricular dysfunction',\n",
       " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       " 'rating': 9.0,\n",
       " 'date': 'May 20, 2012',\n",
       " 'usefulCount': 27}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so filter out null values with lambda function\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "\n",
    "# now we can lowercase\n",
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "\n",
    "drug_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c20381-98ea-4954-8426-30b0b5ac43c5",
   "metadata": {},
   "source": [
    "# Creating new columns (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5891b7f-06fd-48cb-93a6-c9a4c24c81b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': 206461,\n",
       " 'drugName': 'Valsartan',\n",
       " 'condition': 'left ventricular dysfunction',\n",
       " 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"',\n",
       " 'rating': 9.0,\n",
       " 'date': 'May 20, 2012',\n",
       " 'usefulCount': 27,\n",
       " 'review_length': 17}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create function to compute new column value\n",
    "# Returns a dict with the new key:value pair to add to each example\n",
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())}\n",
    "\n",
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "drug_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d6e89-0e07-40b2-8b43-78bdf32e0498",
   "metadata": {},
   "source": [
    "# Sorting\n",
    "Allows you to see extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b6cdc03-ad50-4102-8399-cd82e39bcf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patient_id': [111469, 13653, 53602],\n",
       " 'drugName': ['Ledipasvir / sofosbuvir',\n",
       "  'Amphetamine / dextroamphetamine',\n",
       "  'Alesse'],\n",
       " 'condition': ['hepatitis c', 'adhd', 'birth control'],\n",
       " 'review': ['\"Headache\"', '\"Great\"', '\"Awesome\"'],\n",
       " 'rating': [10.0, 10.0, 10.0],\n",
       " 'date': ['February 3, 2015', 'October 20, 2009', 'November 23, 2015'],\n",
       " 'usefulCount': [41, 3, 0],\n",
       " 'review_length': [1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_dataset[\"train\"].sort(\"review_length\")[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1877467-bf56-421e-bfe6-151706835741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 138514, 'test': 46108}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review that are short are not helpful so let's filter them out\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\n",
    "drug_dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a22c2-1810-4f33-94b8-0f01d1c623e2",
   "metadata": {},
   "source": [
    "# Escaping HTML characters\n",
    "Use python `html` module's `unescape()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aaebfed-953f-49f7-ae44-67cf36585f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm a giant tool\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "text = \"I&#039;m a giant tool\"\n",
    "html.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fdf1e09-cf50-4001-9529-c5b5fc3b0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f765a43-cd93-4330-881e-469a88433dd4",
   "metadata": {},
   "source": [
    "# Speeding up `.map()` calls\n",
    "Using the `batched=True` argument makes it run faster \n",
    "(because it operates on a list of values at the same time, not just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a650f1c9-2617-4a0b-8af2-db251746cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_drug_dataset = drug_dataset.map(lambda x: {\"review\": [html.unescape(rev) for rev in x[\"review\"]]}, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72219d3d-cc22-49a1-b3ce-58730738db3d",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "Here we will tokenize our examples and truncate them to a maximum length of 128, but we will ask the tokenizer to return all the chunks of the texts instead of just the first one. \n",
    "\n",
    "This can be done with `return_overflowing_tokens=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3994696c-d643-4880-ac0f-9b5a643efa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True, # causes truncated tokens to be returned as another example\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73c2d9bd-12df-47ba-82d1-c449e5a456c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 49]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try in out\n",
    "result = tokenize_and_split(drug_dataset[\"train\"][0])\n",
    "[len(inp) for inp in result[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f7abb84-965a-4beb-aafa-2ca17f9aee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS WILL ERRROR\n",
    "# tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4a65f-6491-4c60-b3ad-d8891d00ac5b",
   "metadata": {},
   "source": [
    "This produced an error because the .map() function deals with 1000 in a batch but the `return_overflowing_tokens=True` caused there to be 1463. However these extra 463 have no values for the other columns.\n",
    "\n",
    "`ArrowInvalid: Column 8 named input_ids expected length 1000 but got length 1463`\n",
    "\n",
    "We need to either \n",
    "1. remove the columns from the old dataset. We can do the former with the `remove_columns` argument\n",
    "2. make them the same size as they are in the new dataset. Use the `overflow_to_sample_mapping` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eebbd74b-4f47-48c1-a4ba-fa1708f81295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206772, 138514)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. remove the columns from the old dataset\n",
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True, \n",
    "                                     remove_columns=drug_dataset[\"train\"].column_names)\n",
    "\n",
    "len(tokenized_dataset[\"train\"]), len(drug_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe4e0fac-2215-4744-9f10-d968eab8f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. make them the same size as they are in the new dataset\n",
    "def tokenize_and_split(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"review\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    # Extract mapping between new and old indices\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee98634c-cf61-4706-99f0-3734d8114f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 206772\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 68876\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This works! it preserves all the data\n",
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c8262f-bed7-457d-bc3d-96c1b9ab3e53",
   "metadata": {},
   "source": [
    "# Converting betweed DataSet and Pandas, etc\n",
    "Under the hood, datasets are Apache Arrow, but we can change the display format, eg:\n",
    "```python\n",
    "drug_dataset.set_format(\"pandas\")\n",
    "```\n",
    "This changes the return format for the dataset’s `__getitem__()` dunder method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e373068-e465-4a31-941c-6f55f4c00b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(drug_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79daeff7-f93a-4d51-aa43-ce73501af62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>drugName</th>\n",
       "      <th>condition</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95260</td>\n",
       "      <td>Guanfacine</td>\n",
       "      <td>adhd</td>\n",
       "      <td>\"My son is halfway through his fourth week of ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>April 27, 2010</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92703</td>\n",
       "      <td>Lybrel</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>December 14, 2009</td>\n",
       "      <td>17</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138000</td>\n",
       "      <td>Ortho Evra</td>\n",
       "      <td>birth control</td>\n",
       "      <td>\"This is my first time using any form of birth...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id    drugName      condition  \\\n",
       "0       95260  Guanfacine           adhd   \n",
       "1       92703      Lybrel  birth control   \n",
       "2      138000  Ortho Evra  birth control   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  \"My son is halfway through his fourth week of ...     8.0   \n",
       "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
       "2  \"This is my first time using any form of birth...     8.0   \n",
       "\n",
       "                date  usefulCount  review_length  \n",
       "0     April 27, 2010          192            141  \n",
       "1  December 14, 2009           17            134  \n",
       "2   November 3, 2015           10             89  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change output format so it returns a dataframe, (even though it is still a dataset)\n",
    "drug_dataset.set_format(\"pandas\")\n",
    "drug_dataset[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5d1aa8f-261e-4d8b-a23e-1fa015b9d6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a pandas.DataFrame for the whole training set by selecting all the elements of drug_dataset[\"train\"]\n",
    "train_df = drug_dataset[\"train\"][:]\n",
    "type(drug_dataset[\"train\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67a56ddc-4a10-424b-b225-dc11f78e3c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>birth control</td>\n",
       "      <td>27655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>depression</td>\n",
       "      <td>8023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acne</td>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>4991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pain</td>\n",
       "      <td>4744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       frequency  count\n",
       "0  birth control  27655\n",
       "1     depression   8023\n",
       "2           acne   5209\n",
       "3        anxiety   4991\n",
       "4           pain   4744"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can do fancy pandas analysis\n",
    "frequencies = (\n",
    "    train_df[\"condition\"]\n",
    "    .value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n",
    ")\n",
    "frequencies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe30ba03-8274-44f6-8ca0-b6f8e8bedbd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['frequency', 'count'],\n",
       "    num_rows: 819\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Convert from pandas df into dataset\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "freq_dataset = Dataset.from_pandas(frequencies)\n",
    "freq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0751761e-9115-4f52-a517-809a8823b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change format back to dataset\n",
    "drug_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c241c6-44a5-4098-9790-490505b8d5bc",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "722415fc-b3cb-4638-a453-c52538570a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a 20% validation set\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "drug_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9124f29-6738-4c30-b01b-bc793401c1a0",
   "metadata": {},
   "source": [
    "# Saving Datasets\n",
    "Can be saved in arrow(default), csv, json, parquet\n",
    "- Arrow:\t`Dataset.save_to_disk()` \n",
    "- CSV\t`Dataset.to_csv()`\n",
    "- JSON\t`Dataset.to_json`\n",
    "EG save cleaned dataset in the Arrow format\n",
    "\n",
    "```python\n",
    "drug_dataset_clean.save_to_disk(\"drug-reiews\")\n",
    "```\n",
    "\n",
    "```tree\n",
    "This will create a directory with the followingtx```treeopied\n",
    "drug-reviews/\n",
    "├── dataset_dict.json\n",
    "├── test\n",
    "│   ├── dataset.arrow\n",
    "│   ├── dataset_info.json\n",
    "│   └── state.json\n",
    "├── train\n",
    "│   ├── dataset.arrow\n",
    "│   ├── dataset_info.json\n",
    "│   ├── indices.arrow\n",
    "│   └── state.json\n",
    "└── validation\n",
    "    ├── dataset.arrow\n",
    "    ├── dataset_info.json\n",
    "    ├── indices.arrow\n",
    "    └── state.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e379afc5-7f8b-42d1-a9b4-0094c1b09bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042460d01291453a9c09515b1e813e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/110811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f72990d43664c6cb03c39f8676ab323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/27703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a9aaea21a04ad58dde23b136eb4380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 110811\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 27703\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
       "        num_rows: 46108\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arrow\n",
    "from datasets import load_from_disk\n",
    "\n",
    "drug_dataset_clean.save_to_disk(\"drug-reviews\")\n",
    "drug_dataset_reloaded = load_from_disk(\"drug-reviews\")\n",
    "drug_dataset_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b56ca81-1345-401d-9b14-69edee642758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58796fd96d504a8c9837ae2499548df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/111 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df070cce1f8455cb3e52687c1be5517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/28 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46bd145f6334863a7ba9d5c362b3000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/47 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869dad16d8d54c21bbe7f65397f02790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d38a0728964e95ad3d298d478bf8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d6feb999134ccb87a2a0f205dd4bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# json (uses jsonlines `.jsonl` format (like a json array but uses `\\n` instead of `,` as separator\n",
    "\n",
    "# save\n",
    "folder = \"drug-reviews-json\"\n",
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"{folder}/drug-reviews-{split}.jsonl\")\n",
    "# load\n",
    "data_files = {\n",
    "    \"train\": f\"{folder}/drug-reviews-train.jsonl\",\n",
    "    \"validation\": f\"{folder}/drug-reviews-validation.jsonl\",\n",
    "    \"test\": f\"{folder}/drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b440ead-dab0-44d9-abbb-98361afd8b82",
   "metadata": {},
   "source": [
    "# Dealing with very large datasets\n",
    "Uses loading and streaming so data doesn't have to all fit in memory. Arrow uses memory mapping to map parts of the dataset onto virtual memory locations so they can be accessed in smaller pieces.\n",
    "\n",
    "**Iterable Dataset** is used when the dataset is so huge that even downloading it is hard. So is it downloaded and used in pieces. Used by setting the `streamoing=True` argument in `load_dataset()`\n",
    "\n",
    "Example: The Pile (825GB) is a huge NLP corpus, available in 14GB chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d2668da-8b83-419e-8c4a-69e7b5869a0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find 'https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# look at the PubMed Abstracts section (This takes a few minutes to run)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m pubmed_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m pubmed_dataset\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hugging\\lib\\site-packages\\datasets\\load.py:2523\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2518\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[0;32m   2519\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2520\u001b[0m )\n\u001b[0;32m   2522\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2523\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2524\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   2525\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2526\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[0;32m   2527\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[0;32m   2528\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2529\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   2530\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[0;32m   2531\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m   2532\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2533\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2534\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2535\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2536\u001b[0m     _require_default_config_name\u001b[38;5;241m=\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2537\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[0;32m   2538\u001b[0m )\n\u001b[0;32m   2540\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hugging\\lib\\site-packages\\datasets\\load.py:2195\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   2194\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 2195\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2207\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hugging\\lib\\site-packages\\datasets\\load.py:1736\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1727\u001b[0m \n\u001b[0;32m   1728\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[0;32m   1730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hugging\\lib\\site-packages\\datasets\\load.py:1120\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1118\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[0;32m   1119\u001b[0m patterns \u001b[38;5;241m=\u001b[39m sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path)\n\u001b[1;32m-> 1120\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1125\u001b[0m supports_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m _MODULE_SUPPORTS_METADATA\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m supports_metadata \u001b[38;5;129;01mand\u001b[39;00m patterns \u001b[38;5;241m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hugging\\lib\\site-packages\\datasets\\data_files.py:689\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[0;32m    686\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    688\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 689\u001b[0m         \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m patterns_for_key\n\u001b[0;32m    697\u001b[0m     )\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hugging\\lib\\site-packages\\datasets\\data_files.py:594\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[1;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    593\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m--> 594\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m         )\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\hugging\\lib\\site-packages\\datasets\\data_files.py:383\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[1;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unable to find 'https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# look at the PubMed Abstracts section (This takes a few minutes to run)\n",
    "# THIS DATASET WAS TAKEN DOWN :(\n",
    "data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
    "pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\n",
    "pubmed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730b4e4-1dfc-4bce-a72f-b83424169b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c055b588-1041-473f-8daa-bca457faebbc",
   "metadata": {},
   "source": [
    "# Assembling a dataset\n",
    "ake dataset of github issues associated with the HF Datasets Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0abdb0d3-5e71-4c79-a90c-43379b708328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # take environment variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c040e067-a6cc-436e-b239-3e134c4cff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the first issue on the first page\n",
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
    "headers = {\"Authorization\": f'token {os.getenv(\"GITHUB_TOKEN\")}'}\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d09b64db-b637-4d1e-b606-dd1f3d323bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/6640',\n",
       "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/6640/labels{/name}',\n",
       "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/6640/comments',\n",
       "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/6640/events',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/issues/6640',\n",
       "  'id': 2115864531,\n",
       "  'node_id': 'I_kwDODunzps5-HYfT',\n",
       "  'number': 6640,\n",
       "  'title': 'Sign Language Support',\n",
       "  'user': {'login': 'Merterm',\n",
       "   'id': 6684795,\n",
       "   'node_id': 'MDQ6VXNlcjY2ODQ3OTU=',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/6684795?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/Merterm',\n",
       "   'html_url': 'https://github.com/Merterm',\n",
       "   'followers_url': 'https://api.github.com/users/Merterm/followers',\n",
       "   'following_url': 'https://api.github.com/users/Merterm/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/Merterm/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/Merterm/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/Merterm/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/Merterm/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/Merterm/repos',\n",
       "   'events_url': 'https://api.github.com/users/Merterm/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/Merterm/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'labels': [{'id': 1935892871,\n",
       "    'node_id': 'MDU6TGFiZWwxOTM1ODkyODcx',\n",
       "    'url': 'https://api.github.com/repos/huggingface/datasets/labels/enhancement',\n",
       "    'name': 'enhancement',\n",
       "    'color': 'a2eeef',\n",
       "    'default': True,\n",
       "    'description': 'New feature or request'}],\n",
       "  'state': 'open',\n",
       "  'locked': False,\n",
       "  'assignee': None,\n",
       "  'assignees': [],\n",
       "  'milestone': None,\n",
       "  'comments': 0,\n",
       "  'created_at': '2024-02-02T21:54:51Z',\n",
       "  'updated_at': '2024-02-02T21:54:51Z',\n",
       "  'closed_at': None,\n",
       "  'author_association': 'NONE',\n",
       "  'active_lock_reason': None,\n",
       "  'body': '### Feature request\\n\\nCurrently, there are only several Sign Language labels, I would like to propose adding all the Signed Languages as new labels which are described in this ISO standard: https://www.evertype.com/standards/iso639/sign-language.html\\n\\n### Motivation\\n\\nDatasets currently only have labels for several signed languages. There are more signed languages in the world. Furthermore, some signed languages that have a lot of online data cannot be found because of this reason (for instance, German Sign Language, and there is no German Sign Language label on huggingface datasets even though there are a lot of readily available sign language datasets exist for German Sign Language, which are used very frequently in Sign Language Processing papers, and models.)\\n\\n### Your contribution\\n\\nI can submit a PR for this as well, adding the ISO codes and languages to the labels in datasets.',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/6640/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/6640/timeline',\n",
       "  'performed_via_github_app': None,\n",
       "  'state_reason': None}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae84c624-f647-492f-a85d-dc63791a6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    num_issues=10_000,\n",
    "    rate_limit=5_000,\n",
    "    issues_path=Path(\".\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100  # Number of issues to return per page\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []  # Flush batch for next time period\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c287f37-8e59-4d24-9fee-4f7433010db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0edfb45aa643838824ead638a793ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached GitHub rate limit. Sleeping for one hour ...\n"
     ]
    }
   ],
   "source": [
    "# This takes a long time as we max ou t the api hourly limit\n",
    "# we'll skip this part and use a prepared dataset :(\n",
    "fetch_issues(issues_path=Path(\"./issues\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7ca0f-8744-4f11-a779-5f2e26875ab8",
   "metadata": {},
   "source": [
    "# Semantic Search\n",
    "We will apply embeddings to this dataset to identify semantically similar issues. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993b2b5-73b9-4cfd-9b7c-f95809519ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e2242-c3f0-44f5-afba-aab663ae8750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79528d3b-9e40-41b9-bc46-87b928e953c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c0fae-b7dc-47d5-958a-59195d515e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8804d-5560-447d-a843-d3a75436d932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c9fb2-9aa6-44bb-8dfa-e296450b2855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hugging]",
   "language": "python",
   "name": "conda-env-hugging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
