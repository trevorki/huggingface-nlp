{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a6ffda-9a0c-4d62-8a1e-505295124851",
   "metadata": {},
   "source": [
    "# Tokenizers II\n",
    "Tokenization occurs in steps:\n",
    "1. Normalization\n",
    "2. Pre-tokenization\n",
    "3. Model\n",
    "4. Postprocessor\n",
    "   \n",
    "# 1. Normalization\n",
    "This is the step in tokenization where the characters are modified to make them understandable to the model. This could include\n",
    "- changing to lowercase\n",
    "- removing accents (beware... depends on language)\n",
    "- mapping unicode characters that look identical onto one unique character cod\n",
    "\n",
    "You can access the tokenizer's normalizer through the `tokenizer.backend_tokenizer.normalizer` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6abce97b-2934-4615-a4a2-58aa9b5f3e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n",
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e38a55-67de-440e-b9cb-b1f90b3d9642",
   "metadata": {},
   "source": [
    "# 2. Pre-tokenization\n",
    "Applies some rules to split the input into words based on spaces and punctuation.\n",
    "\n",
    "You can access the tokenizer's pre-tokenizer through the `tokenizer.backend_tokenizer.pre_tokenizer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84d941d-b7c2-4c98-a393-cd7cf0fa0c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d46557-2f41-46bd-9e46-54f15e017286",
   "metadata": {},
   "source": [
    "Note: This is where the offsets are recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c64f93-9669-4156-8bb1-5b4e982d947c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('Ġhow', (6, 10)),\n",
       " ('Ġare', (10, 14)),\n",
       " ('Ġ', (14, 15)),\n",
       " ('Ġyou', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other tokentizers have different rules\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca29b98-4b04-480e-85bd-a4590c7e8736",
   "metadata": {},
   "source": [
    "This one will split on whitespace and punctuation as well, but it will keep the spaces and replace them with a Ġ symbol, enabling it to recover the original spaces if we decode the tokens. Also note that it did NOT ignore the double space between `are` and `you`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1efedd-6db1-4d8b-82a0-db488bf1791c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁Hello,', (0, 6)),\n",
       " ('▁how', (7, 10)),\n",
       " ('▁are', (11, 14)),\n",
       " ('▁you?', (16, 20))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46b8c42-0355-4c0e-870f-9e083e490da1",
   "metadata": {},
   "source": [
    "Like the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (_), but the T5 tokenizer only splits on whitespace, not punctuation. Also note that it added a space by default at the beginning of the sentence (before Hello) and ignored the double space between `are` and `you`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c3004-66aa-46d1-9e34-2f3b81e15a26",
   "metadata": {},
   "source": [
    "# 3. Tokenizer model\n",
    "There are three main subword tokenization algorithms: \n",
    "- BPE (used by GPT-2 and others)\n",
    "- WordPiece (used for example by BERT)\n",
    "- Unigram (used by T5 and others)\n",
    "\n",
    "![algorithm table](./img/tokenizer_algorithms_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7299055-555e-4e40-9f0b-bc98e43f8903",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE)\n",
    "Break words into subword units that appear frequently in reference corpus. Basic idea:\n",
    "\n",
    "toy corpus: [\"the best is there\"]\n",
    "1. Start with your corpus and split into words, with frequency of each word\n",
    "2. Create a vocabulary with the characters that are in the splits. `vocab=['b', 'e', 'h', 'i', 'r', 's', 't']`\n",
    "4. Count the frequency that every character occurs with every other character throughout all the splits (eg `th`)\n",
    "5. Add the pair(s) of characters with highest frequency to a merge list `merge_list = ['th']` and to the vocabulary `vocab=['b', 'e', 'h', 'i', 'r', 's', 't', th']`\n",
    "6. Remake the splits, replacing pair that were merged with their merged counterpart\n",
    "7. Repeat steps 4-7 until the vocaulary has increased to the desired size.\n",
    "\n",
    "To run tokenizer, the merge rules are applied... and there we have the tokenized text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01645b8e-004a-4d1e-85f2-73670c9aff79",
   "metadata": {},
   "source": [
    "## WordPiece\n",
    "Developd by Google to train BERT. This is very similar to BPE, only it encodes characters that are inside words by prepending `##` to it. Eg `tot` would have the characters `['t', '##o', '##t']`.\n",
    "\n",
    "The pair that are kept is NOT the most frequent, but the one with the highest score given by:\n",
    "$$\\text{score}=\\frac{(\\text{freq of pair})}{\\text{freq of first element} \\times \\text{freq of second element}}$$\n",
    "\n",
    "Unlike BPE, WordPiece saves only the final vocabulary (not the merge rules) so the text is tokenized by finding the longest token that the characters fit into"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f04a54-9415-473a-996e-80966608229a",
   "metadata": {},
   "source": [
    "## Unigram\n",
    "A statistical model based on the probabilities of characters and pairs of characters. Basic idea:\n",
    "1. Compute the probability (proportion) of each pair of characters occurs in the corpus. This leads to a huge vocab\n",
    "2. Calculate the unigram loss\n",
    "3. Remove the one(s) that result in the least increase to the loss\n",
    "4. Repeat steps 1-3 until the vocab has reached the desired size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ebd48-cb4f-4c23-a55b-58d7a449a29b",
   "metadata": {},
   "source": [
    "# Building a tokenizer piece by piece\n",
    "\n",
    "![tokenizer_steps](./img/tokenizer_steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984cf8bb-496e-4dbc-b410-b1c270ceac7f",
   "metadata": {},
   "source": [
    "# Step 1: Acquire a corpus\n",
    "Use the `wikitext-2` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b21cef2-9f4f-4e36-9e02-6b22539331b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da07860f1f34b0fac2c8b65f83ce9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aeebef9587c4f2b83b235565593d598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe6317bd5de4bb39943a86896c832fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dc3227a6604aee9497d5e2d5fcbd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776f606cf8634edd90bc15fa994177f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a124c5375946adb0289ad0df6d6501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb5cd112a684274bf27eae989c39108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# generator that yields batches of 1000 texts\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d088594-fa5e-4185-8bbb-e49b896fec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write to file, incase you're curious\n",
    "# with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for i in range(len(dataset)):\n",
    "#         f.write(dataset[i][\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0cf57-861f-437e-8eba-1d558b8cf679",
   "metadata": {},
   "source": [
    "# 1. WordPiece tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9391257-4c1f-4d01-8f89-a34352c970ca",
   "metadata": {},
   "source": [
    "Start by instantiating a Tokenizer object with a model, then set its normalizer, pre_tokenizer, post_processor, and decoder attributes to the values we want.\n",
    "\n",
    "For this example, use a Tokenizer with a WordPiece model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f839a2-6a45-4c95-bc04-524e60c9f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "# instantiate tokenizer with WordPiece model\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead9d0dd-ba15-4dce-8c7d-38e776a0097c",
   "metadata": {},
   "source": [
    "### Add normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93eac19-f226-461e-b31a-0b68230b77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use BERT normalizer (off-the-shelf)\n",
    "# tokenizer.normalizer = normalizers.BertNormalizer(\n",
    "#     lowercase=True, \n",
    "#     strip_accents=True,\n",
    "#     clean_text = True, # remove all control characters and replace repeating spaces with a single one\n",
    "#     handle_chinese_chars = True # places spaces around Chinese characters\n",
    "# )\n",
    "\n",
    "# OR... Make a BERT-like normalizer from scratch\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(), #  unicode NFD needed so StripAccents properly recognizes accents\n",
    "    normalizers.Lowercase(), \n",
    "    normalizers.StripAccents()\n",
    "])\n",
    "\n",
    "# test it out\n",
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca57786-70ea-40b4-adb6-f245658a8197",
   "metadata": {},
   "source": [
    "### Add pre-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721f3d8-fef8-40f5-b796-a62ced103f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # BERT (off-the-shelf)\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "\n",
    "# # OR... Make a BERT-like pre-tokenizer\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.Whitespace() # splits on whitespace AND puntuation, keeping puctuation\n",
    "\n",
    "# OR compose low lovel ones\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.WhitespaceSplit(), # splits on whitespace ONLY\n",
    "    pre_tokenizers.Punctuation() # splits on punctuation, keeping puctuation\n",
    "])\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")\n",
    "# test it out\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781203ab-92a7-4d4e-974e-7335a3beca91",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d473ce-36b3-4fbb-8584-d8ae9a4eb7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens because they are not in the training corpus\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "\n",
    "# train model\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "561faa67-be41-4c8c-8a77-abbf776cf4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea63ec-3be1-43df-86b4-5072534e018d",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3dac996-55f5-441e-b610-0fd370940631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=20, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12148593-f982-4a59-b61a-35f57113ed90",
   "metadata": {},
   "source": [
    "We don't yet have the post-processing steps that add the `[CLS]` token at the beginning and the `[SEP]` token at the end.\n",
    "\n",
    "We will use a `TemplateProcessor` for this, where we specify how to treat a single sentence and a pair of sentences. For both, we write the special tokens we want to use; the first (or single) sentence is represented by `$A`, while the second sentence (if encoding a pair) is represented by `$B`. For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76a7f290-0673-4937-bf2c-7e4415a67c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)\n",
    "\n",
    "# To mimic the classic BERT template (I DONT'T GET THIS SYNTAX)\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")\n",
    "# test tokenizer\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7703d72-7fd6-4a8a-b7e5-dd6adfc94728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '.', '.', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07da6072-3d14-4a7c-b658-5d1eee0b1232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let ' s test this tokenizer... on a pair of sentences.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## Add decoder\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "\n",
    "# test it out\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9610ebf-15da-4aba-a19a-43341eb812f5",
   "metadata": {},
   "source": [
    "### Save tokenizer\n",
    "We can save the settings as a `.json` file, which will allow us to load it an rebuild it\n",
    "\n",
    "To use it with full functionality, we have to wrap it in `PreTrainedTokenizerFast()` or else it will be slow :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99556c99-9771-41dc-9e02-f27ce13e4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# save parameters to file\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "# load from file\n",
    "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d642f-5c07-4f0d-ad76-79bbaaf9dc9a",
   "metadata": {},
   "source": [
    "### Make tokenizer FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbe0b7-b3f2-44f0-883d-16ae1593e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fast tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c21133-dd9e-4006-a73a-d3925895fe8f",
   "metadata": {},
   "source": [
    "If you are using a specific tokenizer class (like BertTokenizerFast), you will only need to specify the special tokens that are different from the default ones (here, none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80a45d76-fb89-4c92-b91a-2b480bd4b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664bb063-2732-4d81-bf9c-7b6f173454f6",
   "metadata": {},
   "source": [
    "# 2. BPE Tokenizer\n",
    "Like used for GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f115d6b-a07c-46d5-a4d4-d6e06ac226f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ġtest', (5, 10)),\n",
       " ('Ġpre', (10, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# GPT-2 does not use a normalizer, so skip and go directly to the pre-tokenization\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9d03e1d-f48b-49a9-b6be-cf86d39fe173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'et', \"'\", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "# tokenizer.train([\"wikitext-2.txt\"], trainer=trainer) # or from text file\n",
    "\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea9139-ddbe-47b5-960b-de82689066c4",
   "metadata": {},
   "source": [
    "The `trim_offsets = False` option indicates to the post-processor that we should leave the offsets of tokens that begin with `Ġ` as they are: this way the start of the offsets will point to the space before the word, not the first character of the word (since the space is technically part of the token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcb57ada-464f-4bb4-8d33-9e73a3fce9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply  byte-level post-processing \n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69b5eba6-449a-4537-bcd2-04de985c1abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L', 'et', \"'\", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Let's test this tokenizer.\"\n",
    "encoding = tokenizer.encode(sentence)\n",
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1494271-2204-4654-866c-15bd0e431a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's test this tokenizer.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a byte-level decoder\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c762326-bfcc-47ca-8cb7-3c9deccda33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it fast\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "371d0ab9-3ff5-436a-808b-38f8efaad1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or... if the special characters are the same as the gpt2 defaults\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec3850a-4ebf-469e-9496-753c2e58d0c9",
   "metadata": {},
   "source": [
    "# 3. Unigram tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9223c286-3eab-477e-b316-21c2979f0f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"▁Let's\", (0, 5)),\n",
       " ('▁test', (5, 10)),\n",
       " ('▁the', (10, 14)),\n",
       " ('▁pre-tokenizer!', (14, 29))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Regex\n",
    "\n",
    "# instantiate tokenizer\n",
    "tokenizer = Tokenizer(models.Unigram())\n",
    "\n",
    "# add normalizer\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# add pre-tokenizer\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fa70b19-c31b-4b45-b472-2adcddb7b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model, which needs training (XLNet has quite a few special tokens)\n",
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
    ")\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "# tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad2939-878f-470f-8095-5c3c3845c648",
   "metadata": {},
   "source": [
    "A very important argument not to forget for the UnigramTrainer is the `unk_token`. We can also pass along other arguments specific to the Unigram algorithm, such as the `shrinking_factor` for each step where we remove tokens (defaults to 0.75) or the `max_piece_length` to specify the maximum length of a given token (defaults to 16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10ee47df-ecc5-40d3-8dea-59d361e27bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add decoder\n",
    "tokenizer.decoder = decoders.Metaspace()\n",
    "\n",
    "# make it fast\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "\n",
    "# or ...\n",
    "# from transformers import XLNetTokenizerFast\n",
    "# wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8baacaae-4466-4c7c-9ff1-a481a251b321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's test this tokenizer.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0a9a8-8d1e-42e5-8736-b880c1692d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa290f0-0940-4e4d-95b4-f03abafd8c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b1a2b-3dab-4789-8289-34796c52f63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5fa59-873f-4987-9d43-9566774a6092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hugging]",
   "language": "python",
   "name": "conda-env-hugging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
