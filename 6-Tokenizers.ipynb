{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d496a2ba-3e8c-42bf-8100-bf2e597f1f05",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "How to train your own! Tokenizers are deterministic that look at the corpus and decide how to break up text into tokens, so they will necesssarily be different depending on the languag or domain.\n",
    "\n",
    "To avoid using too much RAM we make the dataset an iterator of lists of texts (eg. batches of 100 strings). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e2b55-7fe4-453c-af95-45ff22fce9f4",
   "metadata": {},
   "source": [
    "## CodeSearchNet Python Dataset\n",
    "This was compiled from github libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4eda52ab-d665-494a-858b-811e987f5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a7383b6-4671-48c0-9067-f50330425b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23f8e96a0274f7ca5119911c28bd83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fd9931b22b44aea4221ce76a0958af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81c6797b7b64f319e27b016cb37e403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f7e47bdd9e4bc18e5bea4b2afa13a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48391d7a307544c79e2ab0b02d7ce0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99de6d7aeb7482db1ad66f69aaa076f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This takes a few minutes to load\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9256c9a-ba79-4d74-8757-29e7dbaf3a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1f9f29-e6e7-49ad-90fb-8decb6d6b5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def last_rate_limit(self):\\n        \"\"\"\\n        A `dict` of the rate limit information returned in the most recent\\n        response, or `None` if no requests have been made yet.  The `dict`\\n        consists of all headers whose names begin with ``\"RateLimit\"`` (case\\n        insensitive).\\n\\n        The DigitalOcean API specifies the following rate limit headers:\\n\\n        :var string RateLimit-Limit: the number of requests that can be made\\n            per hour\\n        :var string RateLimit-Remaining: the number of requests remaining until\\n            the limit is reached\\n        :var string RateLimit-Reset: the Unix timestamp for the time when the\\n            oldest request will expire from rate limit consideration\\n        \"\"\"\\n        if self.last_response is None:\\n            return None\\n        else:\\n            return {k:v for k,v in iteritems(self.last_response.headers)\\n                        if k.lower().startswith(\\'ratelimit\\')}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use the `whole_func_string` to train the tokenizer\n",
    "raw_datasets[\"train\"][123456][\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e76905-fa66-4069-9e04-099a6023781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make generator\n",
    "\n",
    "# # BAD! This would make a list of lists that would fill up RAM\n",
    "# training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)]\n",
    "\n",
    "# # GOOD! This makes a generator\n",
    "# training_corpus = (raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000))\n",
    "\n",
    "# BETTER! Define it inside a function\n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9d912-d19e-42ec-b57d-12531189ebbe",
   "metadata": {},
   "source": [
    "### Training an old tokenizer\n",
    "This is faster than starting from scratch so start with the GPT-2 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d3a967b-15cb-4c95-9655-8b5fafd9cd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de7ac8-3518-4e87-b62f-fa6fdb21256d",
   "metadata": {},
   "source": [
    "This tokenizer has a few special symbols, like Ġ and Ċ, which denote spaces and newlines, respectively. As we can see, this is not too efficient: the tokenizer returns individual tokens for each space, when it could group together indentation levels (since having sets of four or eight spaces is going to be very common in code). It also split the function name a bit weirdly, not being used to seeing words with the _ character.\n",
    "\n",
    "As a result we should train it on the new corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bd5b933-f963-4fd4-bfd6-4a44b19af715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: train_new_from_iterator() only works for \"fast\" tokenizers (ie the ones not written in python)\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) # vocab_size=52000\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34ddd3b1-8e02-4935-aa1a-c39213f80331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "164599f2-35e1-422d-ae2a-603d131b1e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-search-net-tokenizer\\\\tokenizer_config.json',\n",
       " 'code-search-net-tokenizer\\\\special_tokens_map.json',\n",
       " 'code-search-net-tokenizer\\\\vocab.json',\n",
       " 'code-search-net-tokenizer\\\\merges.txt',\n",
       " 'code-search-net-tokenizer\\\\added_tokens.json',\n",
       " 'code-search-net-tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save locally\n",
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221afba-766f-42cf-b54a-f4ab02e424d5",
   "metadata": {},
   "source": [
    "# Notes on fast tokenizers\n",
    "In addition to returning `['input_ids', 'token_type_ids', 'attention_mask']`, the tokenizer also can keep track of `offset_mapping`, which refers to which characters the tokens span. Similarly, you can call `.word_ids()` which returns the word the token came from. This is useful for determining if 2 adjacent tokens are part of the same word. While this is easy to see with BERT-type tokenizers (with their ##prefix for tokens that are not at the start of words) it is needed for tokenizers that don't have this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb1e05fe-c0b2-4029-9237-b510d41b7de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8282, 1110, 3776, 1164, 22559, 17260, 1116, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 6), (7, 9), (10, 18), (19, 24), (25, 30), (30, 34), (34, 35), (0, 0)]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"Trevor is learning about tokenizers\"\n",
    "encoding = tokenizer(example, return_offsets_mapping=True)\n",
    "print(type(encoding))\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5846cbc3-78fb-45af-8a17-bfb65938669f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokens</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>Trevor</td>\n",
       "      <td>is</td>\n",
       "      <td>learning</td>\n",
       "      <td>about</td>\n",
       "      <td>token</td>\n",
       "      <td>##izer</td>\n",
       "      <td>##s</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word_ids</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>offset_mapping</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(0, 6)</td>\n",
       "      <td>(7, 9)</td>\n",
       "      <td>(10, 18)</td>\n",
       "      <td>(19, 24)</td>\n",
       "      <td>(25, 30)</td>\n",
       "      <td>(30, 34)</td>\n",
       "      <td>(34, 35)</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            index       0       1       2         3         4         5  \\\n",
       "0          tokens   [CLS]  Trevor      is  learning     about     token   \n",
       "1        word_ids     NaN     0.0     1.0       2.0       3.0       4.0   \n",
       "2  offset_mapping  (0, 0)  (0, 6)  (7, 9)  (10, 18)  (19, 24)  (25, 30)   \n",
       "\n",
       "          6         7       8  \n",
       "0    ##izer       ##s   [SEP]  \n",
       "1       4.0       4.0     NaN  \n",
       "2  (30, 34)  (34, 35)  (0, 0)  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"tokens\":encoding.tokens(),\n",
    "              \"word_ids\": encoding.word_ids(),\n",
    "              \"offset_mapping\": encoding[\"offset_mapping\"]\n",
    "             }).T.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3cc1f-d68e-4407-9efe-5d948b625582",
   "metadata": {},
   "source": [
    "We can map any word or token to characters in the original text, and vice versa, with\n",
    "- `word_to_chars()`\n",
    "- `token_to_chars()`\n",
    "- `char_to_word()`\n",
    "- `char_to_token()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e458f1c6-1710-473e-a018-e376db8df07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokenizers'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's use word #4 (tokenizer)\n",
    "word_id = 4\n",
    "start, end = encoding.word_to_chars(word_id)\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2cb1e4d9-c707-46a7-8d64-352519e6c826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'izer'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = 6 # ##izer\n",
    "start, end = .token_to_chars(token_id)\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b3cb9d4-dfd4-41ab-8519-b7ada1c7d0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.char_to_word(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d421bdf2-2207-4aa6-b0ec-97c48d781f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = 15\n",
    "encoding.char_to_token(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69cacc5-231c-47eb-b8d9-639081f9cc1f",
   "metadata": {},
   "source": [
    "## Inside the token-classification pipeline\n",
    "Recall NER (Named Entity Recognition) with the `pipeline()` function. A pipeline groups together the three stages necessary to get the predictions from a raw text: \n",
    "- tokenization\n",
    "- passing the inputs through the model\n",
    "- post-processing\n",
    "\n",
    "The first two steps in the token-classification pipeline are the same as in any other pipeline, but the post-processing is more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "db744f4d-2ef8-4006-a5d6-cf8a19797dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "token_classifier = pipeline(\"token-classification\", \n",
    "                            model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "32e2830e-0a9b-43ad-a9de-fe49b93c99c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>entity</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.999383</td>\n",
       "      <td>0.998155</td>\n",
       "      <td>0.995907</td>\n",
       "      <td>0.999233</td>\n",
       "      <td>0.973893</td>\n",
       "      <td>0.976115</td>\n",
       "      <td>0.988798</td>\n",
       "      <td>0.993211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>S</td>\n",
       "      <td>##yl</td>\n",
       "      <td>##va</td>\n",
       "      <td>##in</td>\n",
       "      <td>Hu</td>\n",
       "      <td>##gging</td>\n",
       "      <td>Face</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "entity     I-PER     I-PER     I-PER     I-PER     I-ORG     I-ORG     I-ORG   \n",
       "score   0.999383  0.998155  0.995907  0.999233  0.973893  0.976115  0.988798   \n",
       "index          4         5         6         7        12        13        14   \n",
       "word           S      ##yl      ##va      ##in        Hu   ##gging      Face   \n",
       "start         11        12        14        16        33        35        41   \n",
       "end           12        14        16        18        35        40        45   \n",
       "\n",
       "               7  \n",
       "entity     I-LOC  \n",
       "score   0.993211  \n",
       "index         16  \n",
       "word    Brooklyn  \n",
       "start         49  \n",
       "end           57  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "pd.DataFrame(token_classifier(example)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382ee33-fb04-4b7e-8513-d572bf332b6e",
   "metadata": {},
   "source": [
    "This split the words it didn't recognize into tokens, If we want them to be aggregated together we can use the `aggregation_strategy` parameter.\n",
    "\n",
    "The `aggregation_strategy` picked will change the scores computed for each grouped entity. \n",
    "\n",
    "- `simple`: the score is mean of the scores of each token in the given entity: for instance, the score of \"Sylvain\" is the mean of the scores we saw in the previous example for the tokens S, ##yl, ##va, and ##in)\n",
    "- `first`: the score of each entity is the score of the first token of that entity (so for \"Sylvain\" it would be 0.993828, the score of the token \"S\")\n",
    "- `max`: the score of each entity is the maximum score of the tokens in that entity (so for \"Hugging Face\" it would be 0.98879766, the score of \"Face\")\r",
    "- `average`:  the score of each entity is the average of the scores of the words composing that entity (so for “Sylvain” there would be no difference from the `simple` strategy, but \"Hugging Face\" would have a score of 0.9819, the average of the scores for \"Hugging\", 0.975, and \"Face\", 0.98879)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d6fb1789-52ff-4877-918a-d819d76b2ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.998169</td>\n",
       "      <td>Sylvain</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.979602</td>\n",
       "      <td>Hugging Face</td>\n",
       "      <td>33</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.993211</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>49</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score          word  start  end\n",
       "0          PER  0.998169       Sylvain     11   18\n",
       "1          ORG  0.979602  Hugging Face     33   45\n",
       "2          LOC  0.993211      Brooklyn     49   57"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier = pipeline(\"token-classification\", \n",
    "                            model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "                            aggregation_strategy=\"simple\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "pd.DataFrame(token_classifier(example))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53cadea5-3d32-4678-b954-136c7957e184",
   "metadata": {},
   "source": [
    "The aggregation_strategy picked will change the scores computed for each grouped entity. With \"simple\" the score is just the mean of the scores of each token in the given entity: for instance, the score of “Sylvain” is the mean of the scores we saw in the previous example for the tokens S, ##yl, ##va, and ##in. Other strategies available are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e326016-cbdf-42c2-a4ba-8a30111e27f9",
   "metadata": {},
   "source": [
    "## Doing it without the `pipeline()`\n",
    "As in doing the tokenization, running through a model, doing postprocessing separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0927f179-0b64-46c9-a4ae-7e759b4e0903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForTokenClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"tf\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1f44edaa-9a00-43f4-947f-05ab1c3c16b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 19)\n",
      "(1, 19, 9)\n"
     ]
    }
   ],
   "source": [
    "# input_shape = (sequence_length, num_tokens) \n",
    "# ouput_shape = (sequence_length, num_tokens, num_model_labels) \n",
    "\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be60c1e-0494-47f9-924c-6d176d5e5667",
   "metadata": {},
   "source": [
    "Use a softmax function to convert those logits to probabilities, and we take the argmax to get predictions (note that we can take the argmax on the logits because the softmax does not change the order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7c9fc640-800b-45d2-b4e7-7172e3d0727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# generate probabilities (optional... can just use logits directly)\n",
    "probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]\n",
    "probabilities = probabilities.numpy().tolist()\n",
    "\n",
    "# select the max logits as the predicted class\n",
    "predictions = tf.math.argmax(outputs.logits, axis=-1)[0]\n",
    "predictions = predictions.numpy().tolist()\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9f0e0afb-f15e-443b-b8d9-1c8999c7e7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "# What do the predictions mean?\n",
    "label_dict = model.config.id2label\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111221d-de28-4622-8303-9f3016428027",
   "metadata": {},
   "source": [
    "There are 9 prediction labels: \n",
    "\n",
    "- `O`: tokens that are not in any named entity (it stands for \"outside\")\n",
    "- `B-MISC`\n",
    "- `I-MISC`\n",
    "- `B-PER`\n",
    "- `I-PER`\n",
    "- `B-ORG`\n",
    "- `I-ORG`\n",
    "- `B-LOC`\n",
    "- `I-LOC`\n",
    "\n",
    "\n",
    "**Note:**\n",
    "- `B-XXX` means the labels is at the start ot entity XXX's tokens.* It is only used when there are consecutive named entities that need to be separate*d\n",
    "- `I-XXX` means the label is inside entoty XXX's tokens\n",
    "tity XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e39267ed-f2a6-44ab-9485-37054d763871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.999383</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.998155</td>\n",
       "      <td>##yl</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.995907</td>\n",
       "      <td>##va</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-PER</td>\n",
       "      <td>0.999233</td>\n",
       "      <td>##in</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.973893</td>\n",
       "      <td>Hu</td>\n",
       "      <td>33</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.976115</td>\n",
       "      <td>##gging</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>0.988797</td>\n",
       "      <td>Face</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>0.993210</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>49</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity     score      word  start  end\n",
       "0  I-PER  0.999383         S     11   12\n",
       "1  I-PER  0.998155      ##yl     12   14\n",
       "2  I-PER  0.995907      ##va     14   16\n",
       "3  I-PER  0.999233      ##in     16   18\n",
       "4  I-ORG  0.973893        Hu     33   35\n",
       "5  I-ORG  0.976115   ##gging     35   40\n",
       "6  I-ORG  0.988797      Face     41   45\n",
       "7  I-LOC  0.993210  Brooklyn     49   57"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1f322172-0104-44d6-8a39-93b63439b111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.998169</td>\n",
       "      <td>Sylvain</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.979602</td>\n",
       "      <td>Hugging Face</td>\n",
       "      <td>33</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.993210</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>49</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score          word  start  end\n",
       "0          PER  0.998169       Sylvain     11   18\n",
       "1          ORG  0.979602  Hugging Face     33   45\n",
       "2          LOC  0.993210      Brooklyn     49   57"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to group the entities\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71175249-c193-4762-809b-f209879e970e",
   "metadata": {},
   "source": [
    "# Question-Answering Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0907a1e-7863-4a72-ae17-f433c9243e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "366b6fa6-0450-442a-8f20-79ac76976f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "358a1082-11e6-4170-9b0c-8eb11cc9160f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9802601933479309,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "output = question_answerer(question=question, context=context)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "68bea70e-0181-48ab-b71c-72a62235c17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jax, PyTorch, and TensorFlow'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[output[\"start\"]:output[\"end\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de32100-e091-44a9-a9b3-b5510392c5b1",
   "metadata": {},
   "source": [
    "If the context is very long, it might have to be split into pieces to fit into the model. This splitting might cause a problem if it splits along the middle of the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d76fd857-27ae-454e-9f23-0cf24f28a9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jax, PyTorch and TensorFlow'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context = \"\"\"\n",
    "🤗 Transformers: State of the Art NLP\n",
    "\n",
    "🤗 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "output = question_answerer(question=question, context=long_context)\n",
    "long_context[output[\"start\"]:output[\"end\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec01a2c-19dc-4340-bbfa-cb536f67a31a",
   "metadata": {},
   "source": [
    "## Questions Answering without Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "398b3545-6b2d-4cd8-8de9-4e69b9f519f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f4520a43-431b-4099-b0d0-2e57259dff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc356677-aee8-4153-97cd-5d9b9b48b2b7",
   "metadata": {},
   "source": [
    "### QA inputs\n",
    "Note that we tokenize the question and the context as a pair, with the question first, separated by '[SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "27dd7bb8-e3fc-4752-92aa-2185ec7afc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Which', 'deep', 'learning', 'libraries', 'back', '[UNK]', 'Transformers', '?', '[SEP]', '[UNK]', 'Transformers', 'is', 'backed', 'by', 'the', 'three', 'most', 'popular', 'deep', 'learning', 'libraries', '—', 'Jax', ',', 'P', '##y', '##T', '##or', '##ch', ',', 'and', 'Ten', '##sor', '##F', '##low', '—', 'with', 'a', 'sea', '##m', '##less', 'integration', 'between', 'them', '.', 'It', \"'\", 's', 'straightforward', 'to', 'train', 'your', 'models', 'with', 'one', 'before', 'loading', 'them', 'for', 'in', '##ference', 'with', 'the', 'other', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92437c-031a-41b9-959a-0482cf59171a",
   "metadata": {},
   "source": [
    "### QA outputs\n",
    "The QA model has been trained to predict the index of the token starting the answer (here 23) and the index of the token where the answer ends (here 35). This is why those models return 2 tensors: \n",
    "- one for the logits corresponding to the start token of the answer,\n",
    "- one for the logits corresponding to the end token of the answer.\n",
    "\n",
    "Since in this case we have only one input containing 67 tokens, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a8737a99-6be7-4eea-ac42-881164ff28a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 67) (1, 67)\n"
     ]
    }
   ],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "11fb3f60-f810-4e84-9d3a-52d2ff23f436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 67), dtype=float32, numpy=\n",
       "array([[-4.49515   , -6.445369  , -4.711502  , -7.096823  , -7.07262   ,\n",
       "        -7.4981337 , -5.539712  , -4.1367955 , -5.9198594 , -5.419285  ,\n",
       "        -1.5920235 , -1.0857029 , -5.098052  , -2.9330769 , -3.407005  ,\n",
       "         2.2466576 ,  5.1562777 , -1.3601698 , -2.2208617 , -0.96861225,\n",
       "        -4.8112288 , -2.252688  ,  1.4382719 , 10.121094  , -1.5310661 ,\n",
       "         2.2685452 , -1.8951424 , -2.210829  , -4.214192  , -2.5570729 ,\n",
       "        -2.3252347 , -2.604605  ,  1.7046974 , -1.9866575 , -1.7211072 ,\n",
       "        -0.54148585, -2.023939  , -4.42457   , -5.101218  , -4.4965935 ,\n",
       "        -7.893999  , -6.719963  , -4.675866  , -6.3278456 , -4.833928  ,\n",
       "        -5.183854  , -3.372427  , -7.411994  , -8.154199  , -4.4871044 ,\n",
       "        -7.4659348 , -4.329343  , -4.229309  , -3.190328  , -7.9467363 ,\n",
       "        -5.2664833 , -7.590193  , -5.0569754 , -7.4475775 , -7.9083138 ,\n",
       "        -6.595132  , -7.406136  , -8.882107  , -7.6748734 , -6.987933  ,\n",
       "        -7.0465856 , -5.4193087 ]], dtype=float32)>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b1fbe-e4e9-4f82-973a-a5ce29ce4d69",
   "metadata": {},
   "source": [
    "**Masking relevant tokens**: we don't want to pay attention to the `[CLS]` token (though we do care about the `[SEP]` one, since it might indicate the answer is not in the context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1bbe4779-cd36-4f18-8c9c-f160abd54a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = inputs.sequence_ids() # question=1, context=0, specialtokens=None\n",
    "\n",
    "# Mask everything (ie set to True) apart from the tokens of the context (ie set context to False)\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "\n",
    "# # Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "mask = tf.constant(mask)[None] # the [None] changes shape from (67,) to (1,67)\n",
    "\n",
    "# # because we will apply softmax later to get probs, just set masked entries to big negative #\n",
    "\n",
    "start_logits = tf.where(mask, -10000, start_logits) # (condition, value, tensor to apply it to)\n",
    "end_logits = tf.where(mask, -10000, end_logits)     # (condition, value, tensor to apply it to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5d9b1683-7f54-4707-b3bd-f2d35154448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()\n",
    "end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421593dc-3faa-436e-8a59-f379cc5d2f03",
   "metadata": {},
   "source": [
    "At this stage, we could take the argmax of the start and end probabilities — but we might end up with a start index that is greater than the end index, so we need to take a few more precautions. We will compute the probabilities of each possible `start_index` and `end_index` where `start_index <= end_index`, then take the tuple `(start_index, end_index)` with the highest probability.\n",
    "\n",
    "Assuming the events *\"The answer starts at start_index\"* and *\"The answer ends at end_index\"* to be independent, the probability that the answer starts at start_index and ends at end_index is:\n",
    "$$\\text{start\\_probabilities}[\\text{start\\_index}] \\times \\text{end\\_probabilities}[\\text{end\\_index}]$$\n",
    "\n",
    "So, to compute all the scores, we just need to compute all the products where `start_index <= end_index`\n",
    "\n",
    "First let’s compute all the possible products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9012492f-3df5-4e48-b4fd-da8c9bef9568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67, 1), (1, 67))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_probabilities[:, None].shape, end_probabilities[None, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6f601055-3ee2-4a30-bf93-a14aeecd06a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 67)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate matrix of scores\n",
    "scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
    "\n",
    "# Keep only the upper triangular part where start_index>end_index\n",
    "scores = np.triu(scores) #returns upper triangular m\n",
    "\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a89b2005-2389-4b7b-831e-c86a74a7bda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9802602\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_index = scores.argmax().item() # returns index of flattened tensor\n",
    "start_index = max_index // scores.shape[1] # the row of the max\n",
    "end_index = max_index % scores.shape[1]    # the col of the max\n",
    "print(scores[start_index, end_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2094e39-4d64-48f3-85b6-8569e6473fe4",
   "metadata": {},
   "source": [
    "This is the same score we got as when using the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c310b474-9667-4254-82ed-65f25306511d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1576, 23, 35)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index, start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0be7720e-bd68-45f2-8a81-642022137fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Jax, PyTorch, and TensorFlow', 'start': 78, 'end': 106, 'score': 0.9802602}\n"
     ]
    }
   ],
   "source": [
    "# Now map these tokens to words using the offset_mapping of the inputs\n",
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]\n",
    "\n",
    "\n",
    "# format it like pieline result\n",
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff47f4-6ad7-40fa-bdf0-10e53b4033b5",
   "metadata": {},
   "source": [
    "### Compare top 3 from pipeline to the manual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6f80993f-1264-4b0d-9363-e34cebe46b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9802601933479309,\n",
       "  'start': 78,\n",
       "  'end': 106,\n",
       "  'answer': 'Jax, PyTorch, and TensorFlow'},\n",
       " {'score': 0.0082477992400527,\n",
       "  'start': 78,\n",
       "  'end': 108,\n",
       "  'answer': 'Jax, PyTorch, and TensorFlow —'},\n",
       " {'score': 0.0013676979579031467,\n",
       "  'start': 78,\n",
       "  'end': 90,\n",
       "  'answer': 'Jax, PyTorch'}]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline\n",
    "output = question_answerer(question=question, context=context, top_k=3)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "47d83293-6578-49bc-b254-09a64995d95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 35 0.9802602\n",
      "23 36 0.008247784\n",
      "16 35 0.006841465\n"
     ]
    }
   ],
   "source": [
    "# manual\n",
    "top_k = np.argsort(scores, axis = None)[::-1][0:3]\n",
    "for max_index in top_k:\n",
    "    start_index = max_index // scores.shape[1] # the row of the max\n",
    "    end_index = max_index % scores.shape[1]    # the col of the max\n",
    "    print(start_index, end_index, scores[start_index, end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c80208d-dec4-4a23-9a65-6a20b12b6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 2 are the same, but not the 3rd... weird"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c433814-4650-4462-86d8-10fb0f9f5ab1",
   "metadata": {},
   "source": [
    "# Long Contexts\n",
    "If the context is longer than the model can accept, the input will be truncated. For example, our model's `max_length=384`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "32b338de-0830-47b7-a1b7-2d57cd8b5855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321 words in long_context\n",
      "461 tokens in long_context\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(long_context.split(' ' ))} words in long_context\")\n",
    "\n",
    "inputs = tokenizer(question, long_context)\n",
    "print(f\"{len(inputs['input_ids'])} tokens in long_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2ea9b8d0-dfb7-4094-8156-475ec52d4ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internal [SEP]\n"
     ]
    }
   ],
   "source": [
    "# truncate only the context, not the question\n",
    "inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\n",
    "print(tokenizer.decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a5ffe-7d69-46f5-9639-dde89e52b0b4",
   "metadata": {},
   "source": [
    "Note that the answer is not in the context!\n",
    "\n",
    "To overcome this the pipeline can split the context into smaller chunks, with some overlap to avoid splitting on the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "149345e5-eaf2-444f-8d8e-3771471cd88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This sentence is not [SEP]\n",
      "[CLS] is not too long [SEP]\n",
      "[CLS] too long but we [SEP]\n",
      "[CLS] but we are going [SEP]\n",
      "[CLS] are going to split [SEP]\n",
      "[CLS] to split it anyway [SEP]\n",
      "[CLS] it anyway. [SEP]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This sentence is not too long but we are going to split it anyway.\"\n",
    "inputs = tokenizer(\n",
    "    sentence, \n",
    "    truncation=True, \n",
    "    return_overflowing_tokens=True, \n",
    "    max_length=6, # max chunk length\n",
    "    stride=2,  # how many tokens to overlap\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    # print(tokenizer.convert_ids_to_tokens(ids))\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c9fd7b50-6fee-4fd3-a22d-241272eb53f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b1970649-7562-4c0d-a1a9-9a895af52013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] This sentence is not [SEP]\n",
      "[CLS] is not too long [SEP]\n",
      "[CLS] too long but we [SEP]\n",
      "[CLS] but we are going [SEP]\n",
      "[CLS] are going to split [SEP]\n",
      "[CLS] to split it anyway [SEP]\n",
      "[CLS] it anyway. [SEP]\n",
      "[CLS] This sentence is shorter [SEP]\n",
      "[CLS] is shorter but will [SEP]\n",
      "[CLS] but will still get [SEP]\n",
      "[CLS] still get split. [SEP]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# OK, so what is the `overflow_to_sample_mapping`?\n",
    "\n",
    "sentences = [\n",
    "    \"This sentence is not too long but we are going to split it anyway.\",\n",
    "    \"This sentence is shorter but will still get split.\",\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    sentences, \n",
    "    truncation=True, \n",
    "    return_overflowing_tokens=True, \n",
    "    max_length=6, \n",
    "    stride=2\n",
    ")\n",
    "# print out the input chunks\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    # print(tokenizer.convert_ids_to_tokens(ids))\n",
    "    print(tokenizer.decode(ids))\n",
    "\n",
    "#This shows which chunk belongs to which sample\n",
    "print(inputs[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2adfdf-5910-443e-a4f0-9b4b7aff4c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "dba5e408-63dc-49f0-ba04-3720b4b7fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To make this tokenizer behave like the pipeline default\n",
    "## And tokenize `long_context`\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "435ccaa2-62b7-43b8-bb20-fe3fef6b1596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "29616284-6772-4da4-884f-3d8799b40758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384)\n"
     ]
    }
   ],
   "source": [
    "# get rid of `overflow_to_sample_mapping` and `offset_mapping` since they are not used by our model\n",
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "inputs = inputs.convert_to_tensors(\"tf\")\n",
    "print(inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "3be59925-e858-4589-87f4-057951f8984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384) (2, 384)\n"
     ]
    }
   ],
   "source": [
    "# `long_context` was split in two, so after it goes through the model, there are two sets of start and end logits\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "07921538-54d2-42a1-85b4-681bde3a1ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the tokens not in context\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "\n",
    "# Mask all the [PAD] tokens\n",
    "mask = tf.math.logical_or(tf.constant(mask)[None], inputs[\"attention_mask\"] == 0)\n",
    "\n",
    "start_logits = tf.where(mask, -10000, start_logits)\n",
    "end_logits = tf.where(mask, -10000, end_logits)\n",
    "\n",
    "# calculate probs\n",
    "start_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()\n",
    "end_probabilities = tf.math.softmax(end_logits, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4020b3e8-c42b-4821-a779-903df84dd7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 18, 0.33867087960243225), (173, 184, 0.9714869856834412)]\n"
     ]
    }
   ],
   "source": [
    "# do the examining pairs of start and end probabilities\n",
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = np.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41840698-67f3-486f-bc38-fdfcbe84a3c0",
   "metadata": {},
   "source": [
    "Those two candidates correspond to the best answers the model was able to find in each chunk. The model is way more confident the right answer is in the second part (which is a good sign!). Now we just have to map those two token spans to spans of characters in the context (we only need to map the second one to have our answer, but it’s interesting to see what the model has picked in the first chunk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ffcfd65f-5178-4b49-b807-ad94684966b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '\\n🤗 Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867087960243225}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714869856834412}\n"
     ]
    }
   ],
   "source": [
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a56525-50fb-4157-95ab-65be60743308",
   "metadata": {},
   "source": [
    "It was more confident in the secod prediction, which was correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df822b-40c1-4ce8-9d5e-7eb6e38996fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hugging]",
   "language": "python",
   "name": "conda-env-hugging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
