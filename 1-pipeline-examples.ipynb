{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9e40ed-cdcc-411a-82ae-1780c6246e9b",
   "metadata": {},
   "source": [
    "# Huggingface Learning\n",
    "Working through the Huggingface NLP course. When downloading models for the first time it takes a while but they are cached locally in `C:\\Users\\Trevor_Kinsey\\.cache\\huggingface\\hub` so that later instances can reuse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9565a75-323f-4e80-949b-5fbbeee34d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b058536-6078-40d6-8f7d-657a54e31ad0",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "Provide pre-processing, model, and post-processing. Here is a tour of some predefined pipelines built into Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5beb710-8857-4f07-aecd-8bdca30a8024",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6ac7dc-5a42-4f13-bc75-95d9521948d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# instantialte pipeline\n",
    "sentiment = pipeline(task=\"sentiment-analysis\", model = \"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d52ea8-fad3-4bca-bf8b-ada7da91630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9598047137260437}: I've been waiting for a HuggingFace course my whole life.\n",
      "{'label': 'NEGATIVE', 'score': 0.9913671612739563}: looking for a job is soul-crushing\n",
      "{'label': 'NEGATIVE', 'score': 0.9899002909660339}: mashed potatoes, eh?\n",
      "{'label': 'POSITIVE', 'score': 0.9837786555290222}: chocolate, eh?\n"
     ]
    }
   ],
   "source": [
    "statements = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"looking for a job is soul-crushing\",\n",
    "    \"mashed potatoes, eh?\",\n",
    "    \"chocolate, eh?\"\n",
    "]\n",
    "responses = sentiment(statements)\n",
    "for statement, response in zip(statements, responses):\n",
    "    print(f\"{response}: {statement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2a2fe-fe46-40e6-9c8b-3791783ab5d3",
   "metadata": {},
   "source": [
    "# Zero-shot Classification\n",
    "This allows you to specify the labels then classify text by them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "643edbcf-f8b4-4e60-b7e2-716374ba99f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "zero_shot = pipeline(\"zero-shot-classification\", model=\"roberta-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297a7a2a-af2d-4c54-ae62-a1e8c697b3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'cooking', 'dancing'],\n",
       " 'scores': [0.9799639582633972, 0.010605032555758953, 0.009431014768779278]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "zero_shot(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af9de37-6817-4ede-b45e-a10a0744a670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'The CEO had a strong handshake.',\n",
       " 'labels': ['male', 'female'],\n",
       " 'scores': [0.8384835720062256, 0.16151642799377441]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"The CEO had a strong handshake.\"\n",
    "candidate_labels = ['male', 'female']\n",
    "hypothesis_template = \"This text speaks about a {} profession.\"\n",
    "zero_shot(sequence_to_classify, candidate_labels, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580cfde-15b8-478b-84de-ab1416d6fa54",
   "metadata": {},
   "source": [
    "## Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b688a90d-1fde-4f9b-a64c-56811bdee30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\") # note: a bug hallucinates a closing parenthesis in this line of code. WEIRD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22206e0a-3b67-4804-92d6-f6426d2632ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'When you are finished your training you will receive a check of your own for more money. If you need assistance, please let me know.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"When you are finished your training you will\"\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0e6bcb4-4481-4203-9978-b6cd883b1c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'When you are finished your training you will become the type of person who wants to spend the day playing a soccer game. I like to think about it'},\n",
       " {'generated_text': 'When you are finished your training you will notice a big change in the intensity of the first three minutes of your training. You will expect to increase your'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator2 = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator2(\n",
    "    prompt,\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95778007-e978-472e-8505-45ab574e009f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0ba59f4-f70c-484a-ad72-f2f35f8e6585",
   "metadata": {},
   "source": [
    "# Mask Filling\n",
    "This involves filling in the blank in a prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7897429-4be9-4191-9376-40780b049722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model = \"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48380dac-a67c-405c-af56-a6c443c27d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2740415930747986,\n",
       "  'token': 3984,\n",
       "  'token_str': ' wine',\n",
       "  'sequence': 'I like to drink wine with my dinner.'},\n",
       " {'score': 0.0926973968744278,\n",
       "  'token': 3895,\n",
       "  'token_str': ' coffee',\n",
       "  'sequence': 'I like to drink coffee with my dinner.'},\n",
       " {'score': 0.05292801186442375,\n",
       "  'token': 6845,\n",
       "  'token_str': ' tea',\n",
       "  'sequence': 'I like to drink tea with my dinner.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I like to drink <mask> with my dinner.\"\n",
    "unmasker(prompt, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5557cc51-e40d-42a7-89ed-33b2e5f36b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.3800225257873535,\n",
       "   'token': 18882,\n",
       "   'token_str': ' messy',\n",
       "   'sequence': 'I have 3 teenage children so my house can get very messy.'},\n",
       "  {'score': 0.11004576086997986,\n",
       "   'token': 11138,\n",
       "   'token_str': ' crowded',\n",
       "   'sequence': 'I have 3 teenage children so my house can get very crowded.'},\n",
       "  {'score': 0.04842783138155937,\n",
       "   'token': 3610,\n",
       "   'token_str': ' busy',\n",
       "   'sequence': 'I have 3 teenage children so my house can get very busy.'}],\n",
       " [{'score': 0.3566606640815735,\n",
       "   'token': 236,\n",
       "   'token_str': ' want',\n",
       "   'sequence': 'I do want to stay in shape'},\n",
       "  {'score': 0.16791412234306335,\n",
       "   'token': 860,\n",
       "   'token_str': ' try',\n",
       "   'sequence': 'I do try to stay in shape'},\n",
       "  {'score': 0.10354165732860565,\n",
       "   'token': 240,\n",
       "   'token_str': ' need',\n",
       "   'sequence': 'I do need to stay in shape'}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"I have 3 teenage children so my house can get very <mask>.\",\n",
    "    \"I do <mask> to stay in shape\"\n",
    "]\n",
    "unmasker(prompts, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e3207-17dd-4fe2-89a4-04e4d489affe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420e417-acad-4f86-8bc0-e84f14d646b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37ffeb41-b030-4b75-ba7e-dc35ff3b37ff",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "144b2929-ecaa-425a-afd8-cdf0a0ee4ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True, model = \"dbmdz/bert-large-cased-finetuned-conll03-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19ed09ae-75e1-4012-8796-30735a4da8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9983121,\n",
       "  'word': 'Naomi',\n",
       "  'start': 12,\n",
       "  'end': 17},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.80635494,\n",
       "  'word': 'New West Secondary',\n",
       "  'start': 26,\n",
       "  'end': 44},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99673724,\n",
       "  'word': 'New Westminster',\n",
       "  'start': 48,\n",
       "  'end': 63},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99989116,\n",
       "  'word': 'France',\n",
       "  'start': 95,\n",
       "  'end': 101}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My daughter Naomi goes to New West Secondary in New Westminster and is going on an exchange to France next year\"\n",
    "ner(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe1af3-8d9e-45c6-8170-c0f99599985f",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "Answers questions based on a given context, but doesn't use external sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e31319f7-fdaf-4791-a4c6-edb89350f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c11e4155-0723-498c-b27c-5200b6afe857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.5341746807098389, 'start': 27, 'end': 29, 'answer': '47'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"How old am I?\"\n",
    "context=\"My name is Trevor and I am 47 years old.\"\n",
    "question_answerer(question,context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c21ce6-1ddc-4618-9069-89bf28bac947",
   "metadata": {},
   "source": [
    "## Summarizing\n",
    "Makes summary of a text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fd26d04-19a5-4082-a717-9247eb2f8db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model = \"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03a1a13a-907e-44f3-a51c-d03204f245df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the number of graduates in traditional engineering disciplines has declined . in most of the premier american universities engineering curricula now concentrate on and encourage largely the study of engineering science . rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483336a2-d89d-465d-862e-79d52a571778",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0380a02-2234-4699-805f-f2470ef02114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_fr\", model=\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f93ec981-1260-4e3e-833d-fbb3fad315d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Bonjour sir, préparez-vous à une aventure nautique!'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng = \"hello good sir, get ready for a nautical adventure!\"\n",
    "\n",
    "translator(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1530c-42dc-4bf3-b6e8-ced9ec2b0143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7941c95-600d-4837-9ca3-651712ccd2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ebdd53-9dd3-4bad-8199-8bdd30c238f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c274216-68c0-40b3-a581-fda0f95e815d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddea4a9-c500-4fe0-ba7e-c62614beea34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hugging]",
   "language": "python",
   "name": "conda-env-hugging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
