{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c3681e-8781-4b97-87df-60187f26f50b",
   "metadata": {},
   "source": [
    "# Transformers Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fadec5-34ec-4768-8697-ec3d7ff940e7",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "## AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca8dcae-59cf-44b8-a71b-2f7bb66d3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4067616-3f89-436e-8506-3d4660c54f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7cc331-bc4d-44ce-8a85-f61f54049313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\n",
       "array([[  101,  6886,  1037,  5860,  2003,  2028,  1997,  1996,  2995,\n",
       "        26552,  1999,  2166,   102],\n",
       "       [  101,  3974,  2026,  5860,  1999,  1996,  5249,  2003, 25198,\n",
       "          102,     0,     0,     0]])>, 'attention_mask': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"Throwing a disc is one of the true pleasures in life\",\n",
    "    \"Losing my disc in the woods is frustrating\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c45d1f-2194-462c-accb-11e4f61899bf",
   "metadata": {},
   "source": [
    "## Tokenizer without AutoTokenizer\n",
    "If you want to have more control over the tokenizer you can load a specific one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5afd0766-2b22-404a-b8fb-e94b35ec09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# this does the same thing\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5d119d5-313e-42bb-a801-195d69e5c0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")==bert_tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7a4d4e3-ce67-4346-8581-ada371146260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f0cb028-27a4-4197-936e-9dac66dfe0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert-base-cased-tokenizer\\\\tokenizer_config.json',\n",
       " 'bert-base-cased-tokenizer\\\\special_tokens_map.json',\n",
       " 'bert-base-cased-tokenizer\\\\vocab.txt',\n",
       " 'bert-base-cased-tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving local tokenizer\n",
    "folder = \"bert-base-cased-tokenizer\"\n",
    "bert_tokenizer.save_pretrained(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a79091-f4ca-45e2-9a1d-1f11edc88a13",
   "metadata": {},
   "source": [
    "## Tokenizer Steps\n",
    "Calling `tokenizer.tokenize()` takes several discrete steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2dc8848-c351-4329-954e-2a49c009e52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple', '!']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"Using a Transformer network is simple!\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c27668b0-4c20-4ffd-8513-986b131f3494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7993, 170, 13809, 23763, 2443, 1110, 3014, 106]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now look at it in steps\n",
    "\n",
    "# 1 converts tokens to the ids of the vocabular\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e5942a9-8816-424f-8947-7a2ce3165fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Add special tokens\n",
    "model_inputs = tokenizer.prepare_for_model(ids)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e10c64e-7d81-445e-b2cd-46751f319339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Using a Transformer network is simple! [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 3 Decode ids (done with model outputs to convert back into words)\n",
    "decoded_string = tokenizer.decode(model_inputs[\"input_ids\"])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa2054-58ee-4e90-b043-f71e917dd95f",
   "metadata": {},
   "source": [
    "## Padding \n",
    "When batching multiple sequences of different lengths, the shorter ones must be padded with null characters to make it the same length as the longest one.\n",
    "\n",
    "Note: if you pass the padded short sequnce to a predictive model, it will not produce the same output as an unpadded one. BUT this is why we have the attention mask, which tells the model to ignore the pad tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9818cb2-839e-4e8d-8f3a-98be93d4db71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e332a5e-1845-4c5b-afb3-6316dac34eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9fe9d421-205f-4845-899d-c62f291af65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "sequences = [\n",
    "    \"This is a long sequence, as you can see.\", \n",
    "    \"This is short.\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sequences, padding=True)\n",
    "print(model_inputs['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dcc1fc60-aa14-419c-942d-25cbb3716bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "print(model_inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d3c324a-4c9b-4703-b10c-650f2bc5d48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences up to the model max length(512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "print(model_inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3526861-e76d-4846-9de2-5b2d255b2f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
    "print(model_inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c98b5db-7a7b-49a7-b036-46b6064da606",
   "metadata": {},
   "source": [
    "### Truncate Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69262404-8335-4174-b59b-157779f26fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# truncate the sequences that are longer than the model max length (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "print(model_inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4794c02-a23c-48ad-89e1-dd5d484f846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)\n",
    "print(model_inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c317aa9-251f-47cc-ab4f-6fa978714777",
   "metadata": {},
   "source": [
    "# AutoModel\n",
    "This allows us to download a pretrained model with weights EXCLUDING the head using the `from_pretrained()` method. This model accepts the tokenized inputs and outputs features (ie  embeddings) for each input token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b584a6e-541a-4960-babd-4e127111c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903ef6dc-42ca-45a8-8913-5092ae30c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Trevor_Kinsey\\miniconda3\\envs\\hugging\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertModel at 0x21e9d0a23b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModel.from_pretrained(checkpoint)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b005b1f-7750-4999-901d-065c8bdb6593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66362880 (253.15 MB)\n",
      "Trainable params: 66362880 (253.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cd237ea-e91c-40d0-b04e-7a4a4396ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tokenized input through the model to get output\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6689c70-a6c7-41ba-b22a-b1b929af3f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 13, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape is (batch_size, sequence_length, hidden size)\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a198f1a4-a899-466a-b256-784e5deb9aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 13, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92833409-6ee8-48e0-9019-9a5c0b729db2",
   "metadata": {},
   "source": [
    "Similar to AutoModel, which returns the hidden states, there are AutoX for other tasks:\n",
    "- AutoModelForCausalML\n",
    "- AutoModelForMaskedLM\n",
    "- AutoModelForMultipleChoice\n",
    "- AutoModelForQuestionAnswering\n",
    "- AutoModelForSequenceClassification\n",
    "- AutoModelForTokenClassification\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c05225e-ee57-4e3d-858b-0da800360459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a776588b-02be-40a7-adbb-2dc4fd41844c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict outputs\n",
    "outputs = model(inputs)\n",
    "outputs.logits.shape   # shape=(batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a28aa3-2d17-4fb3-8b36-06a73cc21224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-4.1785216,  4.487308 ],\n",
       "       [ 4.3378005, -3.5411556]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3abcf0a2-9e57-448a-aebc-f5ba26426483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What do the logit positions represent?\n",
    "label_dict = model.config.id2label\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b35783f-7add-4892-b179-7233e128087c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.7234674e-04, 9.9982762e-01],\n",
       "       [9.9962151e-01, 3.7848490e-04]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = tf.math.softmax(outputs.logits, axis=-1).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fe18da0-a912-48c6-a8c0-c8697722ef83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = np.argmax(predictions, axis = -1)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "321afe37-bc56-40c2-8814-82c13342823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE Throwing a disc is one of the true pleasures in life\n",
      "NEGATIVE Losing my disc in the woods is frustrating\n"
     ]
    }
   ],
   "source": [
    "for pred, raw_input in zip(predicted_labels, raw_inputs):\n",
    "    print(label_dict[pred], raw_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792c37e-d8ff-45b2-9565-292bfc2e8359",
   "metadata": {},
   "source": [
    "# Models without AutoModel\n",
    "\n",
    "This gives you more control over the specific model/checkpoint you want to use. Concepts:\n",
    "\n",
    "A Model is made up of: \n",
    "- `Config` object that has the details/parameters needed to build the model and instantiate the `Model` class\n",
    "- `Model` class (the architecture but no weights\n",
    "- model file containing te weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ef8b629-30df-4a91-8f02-c67049c53ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeca6e9-d530-4cac-8448-08aa7f4a3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 1: initialize untrained model based on config\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config (THIS WILL HAVE RANDOM WEIGHTS BECAUSE IT\"S NOT USING ANY PRETRAINED WEIGHTS\n",
    "model = TFBertModel(config)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13eb4eaf-46cd-4c17-b5e4-7bcfb24a4e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108310272 (413.17 MB)\n",
      "Trainable params: 108310272 (413.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# METHOD 1: initialize model based with pretrained weights\n",
    "model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df4b480-37e1-45b1-956a-c0dfc910ef2b",
   "metadata": {},
   "source": [
    "## Saving model locally to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b889df24-eefb-4795-a659-d07496fdd3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"bert-base-cased\"\n",
    "model.save_pretrained(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0c00685-367d-4fd5-a9ce-37c5a9f1509b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[ 101, 7592,  999,  102],\n",
       "       [ 101, 4658, 1012,  102],\n",
       "       [ 101, 3835,  999,  102]])>, 'attention_mask': <tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1],\n",
       "       [1, 1, 1, 1],\n",
       "       [1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run model\n",
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "encoded_sequences  = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "encoded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92e6f6bd-f94f-4b7e-ad37-a4ac32bc868e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[ 101, 7592,  999,  102],\n",
       "       [ 101, 4658, 1012,  102],\n",
       "       [ 101, 3835,  999,  102]])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tf.constant(encoded_sequences.input_ids.numpy())\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4193538c-97a2-4e6e-806c-3e6cd51f59e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 4, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(model_inputs)\n",
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00484141-6368-469d-8c4e-a932d109fe01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02543c-38e3-40c0-8420-82ce48cd8143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0b5fcd5-f8d3-4c66-8535-4e46effbe489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "## All together now\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5524ba64-b88b-4690-b5a6-46fa988c12ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-1.5606984,  1.6122835],\n",
       "       [-3.6183178,  3.9137492]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02f2da-8879-4d06-ac30-d22de7080a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hugging]",
   "language": "python",
   "name": "conda-env-hugging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
